{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gharchive_data/2023-09-25-0.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-0.json.gz\n",
      "Downloading gharchive_data/2023-09-25-1.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-1.json.gz\n",
      "Downloading gharchive_data/2023-09-25-2.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-2.json.gz\n",
      "Downloading gharchive_data/2023-09-25-3.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-3.json.gz\n",
      "Downloading gharchive_data/2023-09-25-4.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-4.json.gz\n",
      "Downloading gharchive_data/2023-09-25-5.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-5.json.gz\n",
      "Downloading gharchive_data/2023-09-25-6.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-6.json.gz\n",
      "Downloading gharchive_data/2023-09-25-7.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-7.json.gz\n",
      "Downloading gharchive_data/2023-09-25-8.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-8.json.gz\n",
      "Downloading gharchive_data/2023-09-25-9.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-9.json.gz\n",
      "Downloading gharchive_data/2023-09-25-10.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-10.json.gz\n",
      "Downloading gharchive_data/2023-09-25-11.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-11.json.gz\n",
      "Downloading gharchive_data/2023-09-25-12.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-12.json.gz\n",
      "Downloading gharchive_data/2023-09-25-13.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-13.json.gz\n",
      "Downloading gharchive_data/2023-09-25-14.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-14.json.gz\n",
      "Downloading gharchive_data/2023-09-25-15.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-15.json.gz\n",
      "Downloading gharchive_data/2023-09-25-16.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-16.json.gz\n",
      "Downloading gharchive_data/2023-09-25-17.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-17.json.gz\n",
      "Downloading gharchive_data/2023-09-25-18.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-18.json.gz\n",
      "Downloading gharchive_data/2023-09-25-19.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-19.json.gz\n",
      "Downloading gharchive_data/2023-09-25-20.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-20.json.gz\n",
      "Downloading gharchive_data/2023-09-25-21.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-21.json.gz\n",
      "Downloading gharchive_data/2023-09-25-22.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-22.json.gz\n",
      "Downloading gharchive_data/2023-09-25-23.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-23.json.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the event which is triggered if someone is creating a repository is CreateEvent. i can collect all of the createvents and than i extract the user name from each event.\n",
    "\n",
    "massive existing dataset: https://www.kaggle.com/datasets/johntukey/github-dataset (https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T6ZRJT/5MJZAZ)\n",
    "\n",
    "if i finish this it will be the largest public dataset of its kind. \n",
    "\n",
    "lets write a script which is downloadfing th ewlast month from github \n",
    "\"\"\"\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"Download a file from a specified URL and save it locally.\"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        if r.status_code == 200:\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {local_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {local_filename}\")\n",
    "\n",
    "def generate_urls_for_one_day(year, month, day):\n",
    "    \"\"\"Generate URLs for every hour in a specified day.\"\"\"\n",
    "    base_url = \"https://data.gharchive.org/\"\n",
    "    urls = [f\"{base_url}{year}-{month:02d}-{day:02d}-{hour}.json.gz\" for hour in range(24)]\n",
    "    return urls\n",
    "\n",
    "def main(year, month, day):\n",
    "    urls = generate_urls_for_one_day(year, month, day)\n",
    "    for url in urls:\n",
    "        filename = os.path.join('gharchive_data', url.split('/')[-1])\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        download_file(url, filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('gharchive_data', exist_ok=True)\n",
    "    # Define the specific day you want to download data for\n",
    "    year = 2023\n",
    "    month = 9\n",
    "    day = 25\n",
    "    main(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON objects: 150546\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_count_objects(filename):\n",
    "    \"\"\"Load a JSON file and count the number of JSON objects in it, assuming each line is a separate JSON object.\"\"\"\n",
    "    count = 0\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json.loads(line)  # Try to parse each line as a separate JSON object\n",
    "                count += 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    print(f\"Number of JSON objects: {count}\")\n",
    "\n",
    "# Example usage\n",
    "load_and_count_objects('gharchive_data/2023-09-25-0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my goal essentially is to iterate over all of this files and than extract the usernames and than save them somewhere. i can save each day inside one file, maybe even month\n",
    "#would be interesting to have a chat interface for all the data on pantheon for - based on the datasets i have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '32063910505', 'type': 'PushEvent', 'actor': {'id': 77458451, 'login': 'tobii-dev', 'display_login': 'tobii-dev', 'gravatar_id': '', 'url': 'https://api.github.com/users/tobii-dev', 'avatar_url': 'https://avatars.githubusercontent.com/u/77458451?'}, 'repo': {'id': 696027456, 'name': 'tobii-dev/blur-lua-discord-rpc', 'url': 'https://api.github.com/repos/tobii-dev/blur-lua-discord-rpc'}, 'payload': {'repository_id': 696027456, 'push_id': 15162739297, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/main', 'head': '6ee81ef84d480004cf055a17542d1dd8a28bf9ab', 'before': '76e0e214fa558209cee21d21363bf1567c4e9a8a', 'commits': [{'sha': '6ee81ef84d480004cf055a17542d1dd8a28bf9ab', 'author': {'email': '77458451+tobii-dev@users.noreply.github.com', 'name': 'tobii-dev'}, 'message': 'Update README.md', 'distinct': True, 'url': 'https://api.github.com/repos/tobii-dev/blur-lua-discord-rpc/commits/6ee81ef84d480004cf055a17542d1dd8a28bf9ab'}]}, 'public': True, 'created_at': '2023-09-25T00:00:00Z'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_print_first_object(filename):\n",
    "    \"\"\"Load a JSON file and print the first JSON object.\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_object = json.loads(line)  # Try to parse each line as a separate JSON object\n",
    "                print(json_object)  # Print the first JSON object\n",
    "                break  # Exit after printing the first object\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Example usage\n",
    "load_and_print_first_object('gharchive_data/2023-09-25-0.json')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "what is the best way for getting the this data now? \n",
    "\n",
    "lets iterate over this god damn shit\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#if i efficiently want to do this than i have to go through all the events and i collect the user name lmao \n",
    "\n",
    "#okay i will get the actor data from each link and than i will save it to a csv file without any duplicates - this is going to be lots of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all i am trying to get is {\"id\":\"32063910581\",\"type\":\"CreateEvent\",\"actor\":{\"id\":133305540,\"login\":\"Zzeflup\",\"display_login\":\"Zzeflup\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/Zzeflup\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/133305540?\"},\"repo\":{\"id\":689770307,\"name\":\"MercurialCarp28/SCD\",\"url\":\"https://api.github.com/repos/MercurialCarp28/SCD\"},\"payload\":{\"ref\":\"RamardaZzeflup\",\"ref_type\":\"branch\",\"master_branch\":\"main\",\"description\":\"Sistema Control Docente\",\"pusher_type\":\"user\"},\"public\":true,\"created_at\":\"2023-09-25T00:00:00Z\"}\n",
    "#getting the actor url and thats it for now\n",
    "#once i have that i can send a request later on. now all what counts is god damn speed motherfuckers godspeed motherfuckers\n",
    "\n",
    "#if someone has 20 repos than its likely that he is cracked motherfucker - this is going to be a fun thing to do. we are cooking motherfucker we are cooking so much its incredible\n",
    "#once i have all of below data i can make a really nice global map of developers\n",
    "#lets hire people for this \n",
    "# {\n",
    "#   \"login\": \"Zzeflup\",\n",
    "#   \"id\": 133305540,\n",
    "#   \"node_id\": \"U_kgDOB_IUxA\",\n",
    "#   \"avatar_url\": \"https://avatars.githubusercontent.com/u/133305540?v=4\",\n",
    "#   \"gravatar_id\": \"\",\n",
    "#   \"url\": \"https://api.github.com/users/Zzeflup\",\n",
    "#   \"html_url\": \"https://github.com/Zzeflup\",\n",
    "#   \"followers_url\": \"https://api.github.com/users/Zzeflup/followers\",\n",
    "#   \"following_url\": \"https://api.github.com/users/Zzeflup/following{/other_user}\",\n",
    "#   \"gists_url\": \"https://api.github.com/users/Zzeflup/gists{/gist_id}\",\n",
    "#   \"starred_url\": \"https://api.github.com/users/Zzeflup/starred{/owner}{/repo}\",\n",
    "#   \"subscriptions_url\": \"https://api.github.com/users/Zzeflup/subscriptions\",\n",
    "#   \"organizations_url\": \"https://api.github.com/users/Zzeflup/orgs\",\n",
    "#   \"repos_url\": \"https://api.github.com/users/Zzeflup/repos\",\n",
    "#   \"events_url\": \"https://api.github.com/users/Zzeflup/events{/privacy}\",\n",
    "#   \"received_events_url\": \"https://api.github.com/users/Zzeflup/received_events\",\n",
    "#   \"type\": \"User\",\n",
    "#   \"site_admin\": false,\n",
    "#   \"name\": null,\n",
    "#   \"company\": null,\n",
    "#   \"blog\": \"\",\n",
    "#   \"location\": null,\n",
    "#   \"email\": null,\n",
    "#   \"hireable\": null,\n",
    "#   \"bio\": null,\n",
    "#   \"twitter_username\": null,\n",
    "#   \"public_repos\": 1,\n",
    "#   \"public_gists\": 0,\n",
    "#   \"followers\": 2,\n",
    "#   \"following\": 0,\n",
    "#   \"created_at\": \"2023-05-12T03:15:11Z\",\n",
    "#   \"updated_at\": \"2024-05-16T17:48:07Z\"\n",
    "# }\n",
    "\n",
    "#i need to push an update motherfucker\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1. download data for a day \n",
    "2. iterate over data and extract all actor urls and write actor url to file \n",
    "3. delete file if done\n",
    "4. do this as async as possible\n",
    "5. remove all duplicates\n",
    "6. find a good way for sending requests the collected names and than saving this names to be able to extract values later\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2015-02-15/2015-02-15-0.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-0.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-0.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-0.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-0.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-0.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-0.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-0.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-0.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-0.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-0.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-0.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-0.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-0.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-0.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-0.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-1.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-0.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-0.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-1.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-0.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-0.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-0.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-1.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-1.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-1.json.gz\n",
      "Downloaded 2015-02-27/2015-02-27-0.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-1.json.gz\n",
      "Downloaded 2015-02-24/2015-02-24-0.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-0.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-0.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-0.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-1.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-1.json.gz\n",
      "Downloaded 2015-02-26/2015-02-26-0.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-0.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-1.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-1.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-1.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-2.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-2.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-1.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-2.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-2.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-1.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-1.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-1.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-1.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-1.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-1.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-1.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-3.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-2.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-1.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-3.json.gz\n",
      "Downloaded 2015-02-24/2015-02-24-1.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-2.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-1.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-3.json.gz\n",
      "Downloaded 2015-02-27/2015-02-27-1.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-1.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-1.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-3.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-2.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-2.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-2.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-2.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-1.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-2.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-4.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-1.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-2.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-2.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-2.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-2.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-2.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-4.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-3.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-4.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-2.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-4.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-3.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-2.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-3.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-5.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-3.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-3.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-5.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-2.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-2.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-4.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-3.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-3.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-2.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-3.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-3.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-5.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-3.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-2.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-5.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-4.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-6.json.gz\n",
      "Downloaded 2015-02-26/2015-02-26-1.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-6.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-4.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-2.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-3.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-3.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-3.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-4.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-4.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-5.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-3.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-3.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-3.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-4.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-6.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-4.json.gz\n",
      "Downloaded 2015-02-27/2015-02-27-2.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-2.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-7.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-2.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-5.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-4.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-4.json.gz\n",
      "Downloaded 2015-02-24/2015-02-24-2.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-7.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-4.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-6.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-4.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-3.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-4.json.gz\n",
      "Downloaded 2015-02-26/2015-02-26-2.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-5.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-5.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-6.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-7.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-3.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-5.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-4.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-4.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-3.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-4.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-5.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-8.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-5.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-5.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-4.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-8.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-6.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-4.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-7.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-3.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-5.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-5.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-5.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-3.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-6.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-8.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-9.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-4.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-7.json.gz\n",
      "Downloaded 2015-02-27/2015-02-27-3.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-5.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-6.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-5.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-6.json.gz\n",
      "Downloaded 2015-02-24/2015-02-24-3.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-6.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-4.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-9.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-5.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-5.json.gz\n",
      "Downloaded 2015-02-26/2015-02-26-3.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-5.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-6.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-5.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-8.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-6.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-7.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-9.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-6.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-5.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-6.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-10.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-6.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-4.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-4.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-8.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-6.json.gz\n",
      "Downloaded 2015-02-27/2015-02-27-4.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-10.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-7.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-7.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-9.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-6.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-10.json.gz\n",
      "Downloaded 2015-02-04/2015-02-04-6.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-6.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-7.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-5.json.gz\n",
      "Downloaded 2015-02-13/2015-02-13-7.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-11.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-5.json.gz\n",
      "Downloaded 2015-02-24/2015-02-24-4.json.gz\n",
      "Downloaded 2015-02-18/2015-02-18-7.json.gz\n",
      "Downloaded 2015-02-16/2015-02-16-7.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-7.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-6.json.gz\n",
      "Downloaded 2015-02-25/2015-02-25-6.json.gz\n",
      "Downloaded 2015-02-26/2015-02-26-4.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-5.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-9.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-6.json.gz\n",
      "Downloaded 2015-02-10/2015-02-10-7.json.gz\n",
      "Downloaded 2015-02-23/2015-02-23-7.json.gz\n",
      "Downloaded 2015-02-08/2015-02-08-10.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-11.json.gz\n",
      "Downloaded 2015-02-28/2015-02-28-8.json.gz\n",
      "Downloaded 2015-02-14/2015-02-14-11.json.gz\n",
      "Downloaded 2015-02-27/2015-02-27-5.json.gz\n",
      "Downloaded 2015-02-02/2015-02-02-8.json.gz\n",
      "Downloaded 2015-02-05/2015-02-05-7.json.gz\n",
      "Downloaded 2015-02-20/2015-02-20-8.json.gz\n",
      "Downloaded 2015-02-21/2015-02-21-8.json.gz\n",
      "Downloaded 2015-02-12/2015-02-12-7.json.gz\n",
      "Downloaded 2015-02-09/2015-02-09-7.json.gz\n",
      "Downloaded 2015-02-01/2015-02-01-7.json.gz\n",
      "Downloaded 2015-02-19/2015-02-19-6.json.gz\n",
      "Downloaded 2015-02-24/2015-02-24-5.json.gz\n",
      "Downloaded 2015-02-07/2015-02-07-12.json.gz\n",
      "Downloaded 2015-02-22/2015-02-22-10.json.gz\n",
      "Downloaded 2015-02-03/2015-02-03-6.json.gz\n",
      "Downloaded 2015-02-26/2015-02-26-5.json.gz\n",
      "Downloaded 2015-02-15/2015-02-15-12.json.gz\n",
      "Downloaded 2015-02-11/2015-02-11-6.json.gz\n",
      "Downloaded 2015-02-06/2015-02-06-7.json.gz\n",
      "Downloaded 2015-02-17/2015-02-17-8.json.gz\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import calendar\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"Download a file from a specified URL and save it locally.\"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        if r.status_code == 200:\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {local_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {local_filename}\")\n",
    "\n",
    "def append_gzipped_file_to_output(gzipped_filename, output_filename):\n",
    "    \"\"\"Append the content of a gzipped file to the output file.\"\"\"\n",
    "    with gzip.open(gzipped_filename, 'rb') as f_in:\n",
    "        with open(output_filename, 'ab') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "def download_and_combine_day_data(year, month, day):\n",
    "    \"\"\"Download all hourly data files for a specific day and combine them into a single file.\"\"\"\n",
    "    base_url = \"https://data.gharchive.org/\"\n",
    "    combined_filename = f\"{year}-{month:02d}-{day:02d}.json\"\n",
    "    day_folder = f\"{year}-{month:02d}-{day:02d}\"\n",
    "    \n",
    "    # Create a directory for the day\n",
    "    os.makedirs(day_folder, exist_ok=True)\n",
    "    \n",
    "    # Clear the combined file if it exists\n",
    "    open(os.path.join(day_folder, combined_filename), 'wb').close()\n",
    "    \n",
    "    for hour in range(24):\n",
    "        url = f\"{base_url}{year}-{month:02d}-{day:02d}-{hour}.json.gz\"\n",
    "        local_filename = os.path.join(day_folder, f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
    "        download_file(url, local_filename)\n",
    "        append_gzipped_file_to_output(local_filename, os.path.join(day_folder, combined_filename))\n",
    "    \n",
    "    print(f\"Combined data saved to {os.path.join(day_folder, combined_filename)}\")\n",
    "\n",
    "def extract_actor_urls(input_filename, output_filename):\n",
    "    \"\"\"Extract actor URLs from a JSON file and save them to another file.\"\"\"\n",
    "    actor_urls = set()  # Use a set to avoid duplicates\n",
    "\n",
    "    with open(input_filename, 'r') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                event = json.loads(line)\n",
    "                if 'actor' in event and 'url' in event['actor']:\n",
    "                    actor_urls.add(event['actor']['url'])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "    with open(output_filename, 'w') as outfile:\n",
    "        for url in actor_urls:\n",
    "            outfile.write(url + '\\n')\n",
    "\n",
    "    print(f\"Extracted {len(actor_urls)} unique actor URLs to {output_filename}\")\n",
    "\n",
    "def cleanup_files(day_folder):\n",
    "    \"\"\"Delete all files in the specified day folder.\"\"\"\n",
    "    for file_name in os.listdir(day_folder):\n",
    "        file_path = os.path.join(day_folder, file_name)\n",
    "        os.remove(file_path)\n",
    "    os.rmdir(day_folder)\n",
    "    print(f\"Cleaned up files in {day_folder}\")\n",
    "\n",
    "def download_month_data(year, month):\n",
    "    \"\"\"Download data for each day of a month in parallel, adjusting for the number of days in the month.\"\"\"\n",
    "    days_in_month = calendar.monthrange(year, month)[1]  # Get the number of days in the month\n",
    "    with ThreadPoolExecutor(max_workers=days_in_month) as executor:\n",
    "        futures = [executor.submit(download_and_combine_day_data, year, month, day) for day in range(1, days_in_month + 1)]\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    year = 2015\n",
    "    month = 1  # Example for February\n",
    "\n",
    "    # i am testing the feature that everything is getting deleted after the data is extracted with one day only so that i dont have to wait all the time\n",
    "    \n",
    "    \n",
    "    # Step 1: Download and combine data for the entire month\n",
    "    download_month_data(year, month)\n",
    "    \n",
    "    # Step 2: Extract actor URLs for each day and clean up\n",
    "    for day in range(31, calendar.monthrange(year, month)[1] + 1):\n",
    "        day_folder = f\"{year}-{month:02d}-{day:02d}\"\n",
    "        combined_filename = os.path.join(day_folder, f\"{year}-{month:02d}-{day:02d}.json\")\n",
    "        output_filename = os.path.join(day_folder, \"actor_urls.txt\")\n",
    "        if os.path.exists(combined_filename):\n",
    "            extract_actor_urls(combined_filename, output_filename)\n",
    "            cleanup_files(day_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- need to make sure that no error happens if i have a month like february\n",
    "- need to delete all data after actor urls got extracted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
