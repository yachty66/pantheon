{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gharchive_data/2023-09-25-0.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-0.json.gz\n",
      "Downloading gharchive_data/2023-09-25-1.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-1.json.gz\n",
      "Downloading gharchive_data/2023-09-25-2.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-2.json.gz\n",
      "Downloading gharchive_data/2023-09-25-3.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-3.json.gz\n",
      "Downloading gharchive_data/2023-09-25-4.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-4.json.gz\n",
      "Downloading gharchive_data/2023-09-25-5.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-5.json.gz\n",
      "Downloading gharchive_data/2023-09-25-6.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-6.json.gz\n",
      "Downloading gharchive_data/2023-09-25-7.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-7.json.gz\n",
      "Downloading gharchive_data/2023-09-25-8.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-8.json.gz\n",
      "Downloading gharchive_data/2023-09-25-9.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-9.json.gz\n",
      "Downloading gharchive_data/2023-09-25-10.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-10.json.gz\n",
      "Downloading gharchive_data/2023-09-25-11.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-11.json.gz\n",
      "Downloading gharchive_data/2023-09-25-12.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-12.json.gz\n",
      "Downloading gharchive_data/2023-09-25-13.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-13.json.gz\n",
      "Downloading gharchive_data/2023-09-25-14.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-14.json.gz\n",
      "Downloading gharchive_data/2023-09-25-15.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-15.json.gz\n",
      "Downloading gharchive_data/2023-09-25-16.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-16.json.gz\n",
      "Downloading gharchive_data/2023-09-25-17.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-17.json.gz\n",
      "Downloading gharchive_data/2023-09-25-18.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-18.json.gz\n",
      "Downloading gharchive_data/2023-09-25-19.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-19.json.gz\n",
      "Downloading gharchive_data/2023-09-25-20.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-20.json.gz\n",
      "Downloading gharchive_data/2023-09-25-21.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-21.json.gz\n",
      "Downloading gharchive_data/2023-09-25-22.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-22.json.gz\n",
      "Downloading gharchive_data/2023-09-25-23.json.gz...\n",
      "Downloaded gharchive_data/2023-09-25-23.json.gz\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the event which is triggered if someone is creating a repository is CreateEvent. i can collect all of the createvents and than i extract the user name from each event.\n",
    "\n",
    "massive existing dataset: https://www.kaggle.com/datasets/johntukey/github-dataset (https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T6ZRJT/5MJZAZ)\n",
    "\n",
    "if i finish this it will be the largest public dataset of its kind. \n",
    "\n",
    "lets write a script which is downloadfing th ewlast month from github \n",
    "\n",
    "\"\"\"\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file(url, local_filename):\n",
    "    \"\"\"Download a file from a specified URL and save it locally.\"\"\"\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        if r.status_code == 200:\n",
    "            with open(local_filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Downloaded {local_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {local_filename}\")\n",
    "\n",
    "def generate_urls_for_one_day(year, month, day):\n",
    "    \"\"\"Generate URLs for every hour in a specified day.\"\"\"\n",
    "    base_url = \"https://data.gharchive.org/\"\n",
    "    urls = [f\"{base_url}{year}-{month:02d}-{day:02d}-{hour}.json.gz\" for hour in range(24)]\n",
    "    return urls\n",
    "\n",
    "def main(year, month, day):\n",
    "    urls = generate_urls_for_one_day(year, month, day)\n",
    "    for url in urls:\n",
    "        filename = os.path.join('gharchive_data', url.split('/')[-1])\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        download_file(url, filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('gharchive_data', exist_ok=True)\n",
    "    # Define the specific day you want to download data for\n",
    "    year = 2023\n",
    "    month = 9\n",
    "    day = 25\n",
    "    main(year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON objects: 150546\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_count_objects(filename):\n",
    "    \"\"\"Load a JSON file and count the number of JSON objects in it, assuming each line is a separate JSON object.\"\"\"\n",
    "    count = 0\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json.loads(line)  # Try to parse each line as a separate JSON object\n",
    "                count += 1\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "    print(f\"Number of JSON objects: {count}\")\n",
    "\n",
    "# Example usage\n",
    "load_and_count_objects('gharchive_data/2023-09-25-0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my goal essentially is to iterate over all of this files and than extract the usernames and than save them somewhere. i can save each day inside one file, maybe even month\n",
    "#would be interesting to have a chat interface for all the data on pantheon for - based on the datasets i have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '32063910505', 'type': 'PushEvent', 'actor': {'id': 77458451, 'login': 'tobii-dev', 'display_login': 'tobii-dev', 'gravatar_id': '', 'url': 'https://api.github.com/users/tobii-dev', 'avatar_url': 'https://avatars.githubusercontent.com/u/77458451?'}, 'repo': {'id': 696027456, 'name': 'tobii-dev/blur-lua-discord-rpc', 'url': 'https://api.github.com/repos/tobii-dev/blur-lua-discord-rpc'}, 'payload': {'repository_id': 696027456, 'push_id': 15162739297, 'size': 1, 'distinct_size': 1, 'ref': 'refs/heads/main', 'head': '6ee81ef84d480004cf055a17542d1dd8a28bf9ab', 'before': '76e0e214fa558209cee21d21363bf1567c4e9a8a', 'commits': [{'sha': '6ee81ef84d480004cf055a17542d1dd8a28bf9ab', 'author': {'email': '77458451+tobii-dev@users.noreply.github.com', 'name': 'tobii-dev'}, 'message': 'Update README.md', 'distinct': True, 'url': 'https://api.github.com/repos/tobii-dev/blur-lua-discord-rpc/commits/6ee81ef84d480004cf055a17542d1dd8a28bf9ab'}]}, 'public': True, 'created_at': '2023-09-25T00:00:00Z'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_print_first_object(filename):\n",
    "    \"\"\"Load a JSON file and print the first JSON object.\"\"\"\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_object = json.loads(line)  # Try to parse each line as a separate JSON object\n",
    "                print(json_object)  # Print the first JSON object\n",
    "                break  # Exit after printing the first object\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Example usage\n",
    "load_and_print_first_object('gharchive_data/2023-09-25-0.json')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "what is the best way for getting the this data now? \n",
    "\n",
    "lets iterate over this god damn shit\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#if i efficiently want to do this than i have to go through all the events and i collect the user name lmao \n",
    "\n",
    "#okay i will get the actor data from each link and than i will save it to a csv file without any duplicates - this is going to be lots of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all i am trying to get is {\"id\":\"32063910581\",\"type\":\"CreateEvent\",\"actor\":{\"id\":133305540,\"login\":\"Zzeflup\",\"display_login\":\"Zzeflup\",\"gravatar_id\":\"\",\"url\":\"https://api.github.com/users/Zzeflup\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/133305540?\"},\"repo\":{\"id\":689770307,\"name\":\"MercurialCarp28/SCD\",\"url\":\"https://api.github.com/repos/MercurialCarp28/SCD\"},\"payload\":{\"ref\":\"RamardaZzeflup\",\"ref_type\":\"branch\",\"master_branch\":\"main\",\"description\":\"Sistema Control Docente\",\"pusher_type\":\"user\"},\"public\":true,\"created_at\":\"2023-09-25T00:00:00Z\"}\n",
    "#getting the actor url and thats it for now\n",
    "#once i have that i can send a request later on. now all what counts is god damn speed motherfuckers godspeed motherfuckers\n",
    "\n",
    "#if someone has 20 repos than its likely that he is cracked motherfucker - this is going to be a fun thing to do. we are cooking motherfucker we are cooking so much its incredible\n",
    "#once i have all of below data i can make a really nice global map of developers\n",
    "#lets hire people for this \n",
    "# {\n",
    "#   \"login\": \"Zzeflup\",\n",
    "#   \"id\": 133305540,\n",
    "#   \"node_id\": \"U_kgDOB_IUxA\",\n",
    "#   \"avatar_url\": \"https://avatars.githubusercontent.com/u/133305540?v=4\",\n",
    "#   \"gravatar_id\": \"\",\n",
    "#   \"url\": \"https://api.github.com/users/Zzeflup\",\n",
    "#   \"html_url\": \"https://github.com/Zzeflup\",\n",
    "#   \"followers_url\": \"https://api.github.com/users/Zzeflup/followers\",\n",
    "#   \"following_url\": \"https://api.github.com/users/Zzeflup/following{/other_user}\",\n",
    "#   \"gists_url\": \"https://api.github.com/users/Zzeflup/gists{/gist_id}\",\n",
    "#   \"starred_url\": \"https://api.github.com/users/Zzeflup/starred{/owner}{/repo}\",\n",
    "#   \"subscriptions_url\": \"https://api.github.com/users/Zzeflup/subscriptions\",\n",
    "#   \"organizations_url\": \"https://api.github.com/users/Zzeflup/orgs\",\n",
    "#   \"repos_url\": \"https://api.github.com/users/Zzeflup/repos\",\n",
    "#   \"events_url\": \"https://api.github.com/users/Zzeflup/events{/privacy}\",\n",
    "#   \"received_events_url\": \"https://api.github.com/users/Zzeflup/received_events\",\n",
    "#   \"type\": \"User\",\n",
    "#   \"site_admin\": false,\n",
    "#   \"name\": null,\n",
    "#   \"company\": null,\n",
    "#   \"blog\": \"\",\n",
    "#   \"location\": null,\n",
    "#   \"email\": null,\n",
    "#   \"hireable\": null,\n",
    "#   \"bio\": null,\n",
    "#   \"twitter_username\": null,\n",
    "#   \"public_repos\": 1,\n",
    "#   \"public_gists\": 0,\n",
    "#   \"followers\": 2,\n",
    "#   \"following\": 0,\n",
    "#   \"created_at\": \"2023-05-12T03:15:11Z\",\n",
    "#   \"updated_at\": \"2024-05-16T17:48:07Z\"\n",
    "# }\n",
    "\n",
    "#i need to push an update motherfucker\n",
    "\n",
    "\"\"\"\n",
    "1. download data for a day \n",
    "2. iterate over data and extract all actor urls and write actor url to file \n",
    "3. delete file if done\n",
    "4. do this as async as possible\n",
    "5. remove all duplicates\n",
    "6. find a good way for sending requests the collected names and than saving this names to be able to extract values later\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import aiofiles\n",
    "import gzip\n",
    "import shutil\n",
    "import json\n",
    "import calendar\n",
    "import subprocess\n",
    "import os\n",
    "import asyncio\n",
    "from aiohttp import ClientTimeout\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "#it seems to be quite quick to get to the point o\n",
    "\n",
    "async def download_file(session, url, local_filename, retries=3):\n",
    "    \"\"\"Asynchronously download a file from a specified URL and save it locally with retry mechanism.\"\"\"\n",
    "    timeout = ClientTimeout(total=60*5)  # 5 minutes total timeout\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            async with session.get(url, timeout=timeout) as response:\n",
    "                if response.status == 200:\n",
    "                    content = await response.read()  # Read the whole content at once\n",
    "                    async with aiofiles.open(local_filename, 'wb') as f:\n",
    "                        await f.write(content)\n",
    "                    print(f\"Downloaded {local_filename}\")\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Failed to download {local_filename}, status: {response.status}\")\n",
    "        except (aiohttp.ClientPayloadError, aiohttp.ClientConnectionError, asyncio.TimeoutError) as e:\n",
    "            print(f\"Attempt {attempt + 1} failed with error: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                await asyncio.sleep(2**attempt)  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"Failed to download {local_filename} after {retries} attempts\")\n",
    "        attempt += 1\n",
    "    return False\n",
    "\n",
    "async def download_and_combine_day_data(year, month, day, session):\n",
    "    \"\"\"Asynchronously download all hourly data files for a specific day and combine them into a single file.\"\"\"\n",
    "    base_url = \"https://data.gharchive.org/\"\n",
    "    day_folder = f\"{year}-{month:02d}-{day:02d}\"\n",
    "    combined_filename = f\"{year}-{month:02d}-{day:02d}.json\"\n",
    "    full_path = os.path.join(day_folder, combined_filename)\n",
    "    \n",
    "    os.makedirs(day_folder, exist_ok=True)\n",
    "    \n",
    "    tasks = []\n",
    "    for hour in range(24):\n",
    "        url = f\"{base_url}{year}-{month:02d}-{day:02d}-{hour}.json.gz\"\n",
    "        local_filename = os.path.join(day_folder, f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
    "        task = asyncio.create_task(download_file(session, url, local_filename))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    if any(isinstance(result, Exception) for result in results):\n",
    "        print(f\"Failed to download some files for {day_folder}. Skipping this date.\")\n",
    "        return False\n",
    "    \n",
    "    with open(full_path, 'wb') as f_out:\n",
    "        for hour in range(24):\n",
    "            gz_filename = os.path.join(day_folder, f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
    "            if os.path.exists(gz_filename):  # Check if the file exists before opening\n",
    "                with gzip.open(gz_filename, 'rb') as f_in:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            else:\n",
    "                print(f\"File {gz_filename} does not exist, skipping.\")\n",
    "    return True\n",
    "\n",
    "async def download_month_data(year, month, session):\n",
    "    \"\"\"Asynchronously download data for each day of a month.\"\"\"\n",
    "    days_in_month = calendar.monthrange(year, month)[1]\n",
    "    tasks = [download_and_combine_day_data(year, month, day, session) for day in range(1, days_in_month + 1)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "def extract_actor_urls(input_filename, output_directory, year, month):\n",
    "    \"\"\"Extract actor URLs from a JSON file and append them to a file in the specified directory.\"\"\"\n",
    "    output_filename = os.path.join(output_directory, f\"{year}-{month:02d}-actor-urls.txt\")\n",
    "    actor_urls = set()\n",
    "    with open(input_filename, 'r') as infile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                event = json.loads(line)\n",
    "                if 'actor' in event and 'url' in event['actor']:\n",
    "                    actor_urls.add(event['actor']['url'])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    with open(output_filename, 'a') as outfile:\n",
    "        for url in actor_urls:\n",
    "            outfile.write(url + '\\n')\n",
    "\n",
    "def cleanup_files(day_folder):\n",
    "    \"\"\"Delete all files in the specified day folder using system call, and then empty the Trash.\"\"\"\n",
    "    try:\n",
    "        for file_name in os.listdir(day_folder):\n",
    "            file_path = os.path.join(day_folder, file_name)\n",
    "            subprocess.run(['rm', file_path], check=True)\n",
    "        os.rmdir(day_folder)\n",
    "        print(\"Folder cleaned up.\")\n",
    "        subprocess.run(['osascript', '-e', 'tell app \"Finder\" to empty'], check=True)\n",
    "        print(\"Trash emptied.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during file deletion or emptying trash: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error: {e}\")\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    start_year = 2019\n",
    "    end_year = 2024\n",
    "    output_directory = \"github\"\n",
    "\n",
    "    # Initialize start_month to 10 for the first run\n",
    "    initial_start_month = 10\n",
    "    end_month = 5  # Set to May for the final year\n",
    "\n",
    "    # Create a session using aiohttp's ClientSession\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            # Determine the start month for the current year\n",
    "            if year == start_year:\n",
    "                start_month = initial_start_month\n",
    "            else:\n",
    "                start_month = 1\n",
    "\n",
    "            for month in range(start_month, 13 if year != end_year else end_month + 1):\n",
    "                await download_month_data(year, month, session)  # Pass session to the function\n",
    "                for day in range(1, calendar.monthrange(year, month)[1] + 1):\n",
    "                    day_folder = f\"{year}-{month:02d}-{day:02d}\"\n",
    "                    combined_filename = os.path.join(day_folder, f\"{year}-{month:02d}-{day:02d}.json\")\n",
    "                    success = await download_and_combine_day_data(year, month, day, session)  # Pass session here\n",
    "                    if success and os.path.exists(combined_filename):\n",
    "                        extract_actor_urls(combined_filename, output_directory, year, month)\n",
    "                        cleanup_files(day_folder)\n",
    "                    else:\n",
    "                        print(f\"Skipping processing for {day_folder} due to download issues.\")\n",
    "\n",
    "await main()\n",
    "#one way to do this really quick is if i start a online training of this - i think this would be kinda lets fucking do this. i just spin up 5 different machines on runpod and thna \n",
    "#need a pod for 20,21,22,23,24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "how can i make this process of this faster. in the end what i want is to get all the actor urls for a year\n",
    "\n",
    "since i do batching i could do it so that each year is getting processed in its own batch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in the meanwhile i can figure out how to scrape all the data maybe via proxies to\n",
    "\n",
    "what can i do once i have this data with all the i dont know what to do yet\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trash has been emptied.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def empty_trash():\n",
    "    \"\"\"Empty the Trash on macOS.\"\"\"\n",
    "    try:\n",
    "        subprocess.run(['osascript', '-e', 'tell app \"Finder\" to empty'], check=True)\n",
    "        print(\"Trash has been emptied.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the function to empty the trash\n",
    "empty_trash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2015-01-01-0.json.gz\n",
      "Deleted 2015-01-01-0.json.gz\n",
      "Downloaded 2015-01-01-1.json.gz\n",
      "Deleted 2015-01-01-1.json.gz\n",
      "Downloaded 2015-01-01-2.json.gz\n",
      "Deleted 2015-01-01-2.json.gz\n",
      "Downloaded 2015-01-01-3.json.gz\n",
      "Deleted 2015-01-01-3.json.gz\n",
      "Downloaded 2015-01-01-4.json.gz\n",
      "Deleted 2015-01-01-4.json.gz\n",
      "Downloaded 2015-01-01-5.json.gz\n",
      "Deleted 2015-01-01-5.json.gz\n",
      "Downloaded 2015-01-01-6.json.gz\n",
      "Deleted 2015-01-01-6.json.gz\n",
      "Downloaded 2015-01-01-7.json.gz\n",
      "Deleted 2015-01-01-7.json.gz\n",
      "Downloaded 2015-01-01-8.json.gz\n",
      "Deleted 2015-01-01-8.json.gz\n",
      "Downloaded 2015-01-01-9.json.gz\n",
      "Deleted 2015-01-01-9.json.gz\n",
      "Downloaded 2015-01-01-10.json.gz\n",
      "Deleted 2015-01-01-10.json.gz\n",
      "Downloaded 2015-01-01-11.json.gz\n",
      "Deleted 2015-01-01-11.json.gz\n",
      "Downloaded 2015-01-01-12.json.gz\n",
      "Deleted 2015-01-01-12.json.gz\n",
      "Downloaded 2015-01-01-13.json.gz\n",
      "Deleted 2015-01-01-13.json.gz\n",
      "Downloaded 2015-01-01-14.json.gz\n",
      "Deleted 2015-01-01-14.json.gz\n",
      "Downloaded 2015-01-01-15.json.gz\n",
      "Deleted 2015-01-01-15.json.gz\n",
      "Downloaded 2015-01-01-16.json.gz\n",
      "Deleted 2015-01-01-16.json.gz\n",
      "Downloaded 2015-01-01-17.json.gz\n",
      "Deleted 2015-01-01-17.json.gz\n",
      "Downloaded 2015-01-01-18.json.gz\n",
      "Deleted 2015-01-01-18.json.gz\n",
      "Downloaded 2015-01-01-19.json.gz\n",
      "Deleted 2015-01-01-19.json.gz\n",
      "Downloaded 2015-01-01-20.json.gz\n",
      "Deleted 2015-01-01-20.json.gz\n",
      "Downloaded 2015-01-01-21.json.gz\n",
      "Deleted 2015-01-01-21.json.gz\n",
      "Downloaded 2015-01-01-22.json.gz\n",
      "Deleted 2015-01-01-22.json.gz\n",
      "Downloaded 2015-01-01-23.json.gz\n",
      "Deleted 2015-01-01-23.json.gz\n",
      "Saved actor URLs for 2015-01-01\n",
      "Downloaded 2015-01-02-0.json.gz\n",
      "Deleted 2015-01-02-0.json.gz\n",
      "Downloaded 2015-01-02-1.json.gz\n",
      "Deleted 2015-01-02-1.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2023-01-01-4.json.gz\n",
      "Downloaded 2023-01-01-3.json.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll data combined into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonthly_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Example usage: Process data for January 2015\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mdownload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2015\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#this file needs around 10sec per day or 310 sec per month\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 32\u001b[0m, in \u001b[0;36mdownload_and_process_data\u001b[0;34m(year, month)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/base/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.virtualenvs/base/lib/python3.11/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.virtualenvs/base/lib/python3.11/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/base/lib/python3.11/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1315\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1313\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1314\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from calendar import monthrange\n",
    "\n",
    "def download_and_process_data(year, month):\n",
    "    # Create directories for downloads and results\n",
    "    download_dir = f'gharchive_{year}-{month:02d}'\n",
    "    result_dir = f'results_{year}-{month:02d}'\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "\n",
    "    # Determine the number of days in the month\n",
    "    days_in_month = monthrange(year, month)[1]\n",
    "\n",
    "    for day in range(1, days_in_month + 1):\n",
    "        daily_urls = []  # List to store actor URLs for the day\n",
    "\n",
    "        for hour in range(24):\n",
    "            # Construct the file URL\n",
    "            url = f'https://data.gharchive.org/{year}-{month:02d}-{day:02d}-{hour}.json.gz'\n",
    "            filename = url.split('/')[-1]\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "            # Download the file\n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    for chunk in response.iter_content(chunk_size=128):\n",
    "                        file.write(chunk)\n",
    "                print(f'Downloaded {filename}')\n",
    "\n",
    "                # Process the downloaded file\n",
    "                with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        event_data = json.loads(line)\n",
    "                        if 'actor' in event_data and 'url' in event_data['actor']:\n",
    "                            daily_urls.append(event_data['actor']['url'] + '\\n')\n",
    "\n",
    "                # After processing, delete the file\n",
    "                os.remove(file_path)\n",
    "                print(f'Deleted {filename}')\n",
    "            else:\n",
    "                print(f'Failed to download {filename}')\n",
    "\n",
    "        # Write daily URLs to a text file\n",
    "        daily_file_path = os.path.join(result_dir, f'{year}-{month:02d}-{day:02d}_actor_urls.txt')\n",
    "        with open(daily_file_path, 'w') as f:\n",
    "            f.writelines(daily_urls)\n",
    "        print(f'Saved actor URLs for {year}-{month:02d}-{day:02d}')\n",
    "\n",
    "    # Combine all daily text files into a single monthly file\n",
    "    monthly_file_path = os.path.join(result_dir, f'{year}-{month:02d}_actor_urls.txt')\n",
    "    with open(monthly_file_path, 'w') as monthly_file:\n",
    "        for day in range(1, days_in_month + 1):\n",
    "            daily_file_path = os.path.join(result_dir, f'{year}-{month:02d}-{day:02d}_actor_urls.txt')\n",
    "            with open(daily_file_path, 'r') as daily_file:\n",
    "                monthly_file.writelines(daily_file.readlines())\n",
    "    print(f'All data combined into {monthly_file_path}')\n",
    "\n",
    "# Example usage: Process data for January 2015\n",
    "download_and_process_data(2015, 1)\n",
    "\n",
    "#this file needs around 10sec per day or 310 sec per month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "is it somehow possible to download this data in like 6 hours only? if so - how is that possible, how can i do this? how can i do this pretty fast\n",
    "\n",
    "i cannot wait something like 30 hours for this to finish. this needs to be really fast. i can make for each monht notebook. thats the way to do it actually. \n",
    "\n",
    "so i just going to make a machine for each \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 2015-01-01-9.json.gz\n",
      "Deleted 2015-01-01-9.json.gz\n",
      "Downloaded 2015-01-01-4.json.gz\n",
      "Deleted 2015-01-01-4.json.gz\n",
      "Downloaded 2015-01-01-6.json.gz\n",
      "Downloaded 2015-01-01-10.json.gz\n",
      "Deleted 2015-01-01-6.json.gz\n",
      "Deleted 2015-01-01-10.json.gz\n",
      "Downloaded 2015-01-01-7.json.gz\n",
      "Deleted 2015-01-01-7.json.gz\n",
      "Downloaded 2015-01-01-11.json.gz\n",
      "Downloaded 2015-01-01-2.json.gz\n",
      "Deleted 2015-01-01-11.json.gz\n",
      "Deleted 2015-01-01-2.json.gz\n",
      "Downloaded 2015-01-01-3.json.gz\n",
      "Deleted 2015-01-01-3.json.gz\n",
      "Downloaded 2015-01-01-5.json.gz\n",
      "Downloaded 2015-01-01-1.json.gz\n",
      "Deleted 2015-01-01-5.json.gz\n",
      "Deleted 2015-01-01-1.json.gz\n",
      "Downloaded 2015-01-01-8.json.gz\n",
      "Deleted 2015-01-01-8.json.gz\n",
      "Downloaded 2015-01-01-13.json.gz\n",
      "Downloaded 2015-01-01-12.json.gz\n",
      "Deleted 2015-01-01-12.json.gz\n",
      "Deleted 2015-01-01-13.json.gz\n",
      "Downloaded 2015-01-01-0.json.gz\n",
      "Deleted 2015-01-01-0.json.gz\n",
      "Downloaded 2015-01-01-16.json.gz\n",
      "Deleted 2015-01-01-16.json.gz\n",
      "Downloaded 2015-01-02-0.json.gz\n",
      "Deleted 2015-01-02-0.json.gz\n",
      "Downloaded 2015-01-02-1.json.gz\n",
      "Deleted 2015-01-02-1.json.gz\n",
      "Downloaded 2015-01-02-3.json.gz\n",
      "Deleted 2015-01-02-3.json.gz\n",
      "Downloaded 2015-01-02-16.json.gz\n",
      "Deleted 2015-01-02-16.json.gz\n",
      "Downloaded 2015-01-02-15.json.gz\n",
      "Downloaded 2015-01-02-18.json.gz\n",
      "Deleted 2015-01-02-15.json.gz\n",
      "Deleted 2015-01-02-18.json.gz\n",
      "Downloaded 2015-01-02-17.json.gz\n",
      "Deleted 2015-01-02-17.json.gz\n",
      "Network-related error occurred for 2015-01-02-13.json.gz: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Downloaded 2015-01-02-19.json.gz\n",
      "Deleted 2015-01-02-19.json.gz\n",
      "Downloaded 2015-01-02-20.json.gz\n",
      "Deleted 2015-01-02-20.json.gz\n",
      "Downloaded 2015-01-02-21.json.gz\n",
      "Deleted 2015-01-02-21.json.gz\n",
      "Downloaded 2015-01-02-22.json.gz\n",
      "Deleted 2015-01-02-22.json.gz\n",
      "Downloaded 2015-01-03-0.json.gz\n",
      "Deleted 2015-01-03-0.json.gz\n",
      "Downloaded 2015-01-03-1.json.gz\n",
      "Deleted 2015-01-03-1.json.gz\n",
      "Downloaded 2015-01-03-3.json.gz\n",
      "Downloaded 2015-01-03-2.json.gz\n",
      "Downloaded 2015-01-02-23.json.gz\n",
      "Downloaded 2015-01-03-4.json.gz\n",
      "Deleted 2015-01-03-3.json.gz\n",
      "Deleted 2015-01-03-2.json.gz\n",
      "Deleted 2015-01-03-4.json.gz\n",
      "Deleted 2015-01-02-23.json.gz\n",
      "Downloaded 2015-01-03-5.json.gz\n",
      "Downloaded 2015-01-03-6.json.gz\n",
      "Deleted 2015-01-03-5.json.gz\n",
      "Deleted 2015-01-03-6.json.gz\n",
      "Downloaded 2015-01-03-8.json.gz\n",
      "Deleted 2015-01-03-8.json.gz\n",
      "Downloaded 2015-01-03-7.json.gz\n",
      "Deleted 2015-01-03-7.json.gz\n",
      "Downloaded 2015-01-03-9.json.gz\n",
      "Deleted 2015-01-03-9.json.gz\n",
      "Downloaded 2015-01-03-10.json.gz\n",
      "Deleted 2015-01-03-10.json.gz\n",
      "Downloaded 2015-01-03-12.json.gz\n",
      "Deleted 2015-01-03-12.json.gz\n",
      "Downloaded 2015-01-03-11.json.gz\n",
      "Downloaded 2015-01-03-13.json.gz\n",
      "Deleted 2015-01-03-11.json.gz\n",
      "Deleted 2015-01-03-13.json.gz\n",
      "Downloaded 2015-01-03-14.json.gz\n",
      "Deleted 2015-01-03-14.json.gz\n",
      "Downloaded 2015-01-03-15.json.gz\n",
      "Deleted 2015-01-03-15.json.gz\n",
      "Downloaded 2015-01-03-18.json.gz\n",
      "Downloaded 2015-01-03-16.json.gz\n",
      "Deleted 2015-01-03-18.json.gz\n",
      "Downloaded 2015-01-03-17.json.gz\n",
      "Deleted 2015-01-03-16.json.gz\n",
      "Downloaded 2015-01-03-19.json.gz\n",
      "Deleted 2015-01-03-17.json.gz\n",
      "Deleted 2015-01-03-19.json.gz\n",
      "Downloaded 2015-01-03-20.json.gz\n",
      "Deleted 2015-01-03-20.json.gz\n",
      "Downloaded 2015-01-03-21.json.gz\n",
      "Deleted 2015-01-03-21.json.gz\n",
      "Downloaded 2015-01-03-22.json.gz\n",
      "Downloaded 2015-01-04-0.json.gz\n",
      "Deleted 2015-01-03-22.json.gz\n",
      "Deleted 2015-01-04-0.json.gz\n",
      "Downloaded 2015-01-04-1.json.gz\n",
      "Downloaded 2015-01-03-23.json.gz\n",
      "Deleted 2015-01-04-1.json.gz\n",
      "Deleted 2015-01-03-23.json.gz\n",
      "Downloaded 2015-01-04-3.json.gz\n",
      "Deleted 2015-01-04-3.json.gz\n",
      "Downloaded 2015-01-04-2.json.gz\n",
      "Deleted 2015-01-04-2.json.gz\n",
      "Downloaded 2015-01-04-5.json.gz\n",
      "Deleted 2015-01-04-5.json.gz\n",
      "Downloaded 2015-01-04-4.json.gz\n",
      "Deleted 2015-01-04-4.json.gz\n",
      "Downloaded 2015-01-04-6.json.gz\n",
      "Deleted 2015-01-04-6.json.gz\n",
      "Downloaded 2015-01-04-7.json.gz\n",
      "Deleted 2015-01-04-7.json.gz\n",
      "Downloaded 2015-01-04-9.json.gz\n",
      "Deleted 2015-01-04-9.json.gz\n",
      "Downloaded 2015-01-04-8.json.gz\n",
      "Deleted 2015-01-04-8.json.gz\n",
      "Downloaded 2015-01-04-11.json.gz\n",
      "Deleted 2015-01-04-11.json.gz\n",
      "Downloaded 2015-01-04-10.json.gz\n",
      "Deleted 2015-01-04-10.json.gz\n",
      "Downloaded 2015-01-04-13.json.gz\n",
      "Deleted 2015-01-04-13.json.gz\n",
      "Downloaded 2015-01-04-12.json.gz\n",
      "Deleted 2015-01-04-12.json.gz\n",
      "Downloaded 2015-01-04-14.json.gz\n",
      "Deleted 2015-01-04-14.json.gz\n",
      "Downloaded 2015-01-04-16.json.gz\n",
      "Deleted 2015-01-04-16.json.gz\n",
      "Downloaded 2015-01-04-15.json.gz\n",
      "Deleted 2015-01-04-15.json.gz\n",
      "Downloaded 2015-01-04-19.json.gz\n",
      "Downloaded 2015-01-04-17.json.gz\n",
      "Deleted 2015-01-04-19.json.gz\n",
      "Deleted 2015-01-04-17.json.gz\n",
      "Downloaded 2015-01-04-20.json.gz\n",
      "Downloaded 2015-01-04-18.json.gz\n",
      "Deleted 2015-01-04-20.json.gz\n",
      "Deleted 2015-01-04-18.json.gz\n",
      "Downloaded 2015-01-04-21.json.gzDownloaded 2015-01-04-22.json.gz\n",
      "\n",
      "Downloaded 2015-01-05-0.json.gz\n",
      "Deleted 2015-01-04-22.json.gz\n",
      "Deleted 2015-01-04-21.json.gz\n",
      "Deleted 2015-01-05-0.json.gz\n",
      "Downloaded 2015-01-04-23.json.gz\n",
      "Deleted 2015-01-04-23.json.gz\n",
      "Downloaded 2015-01-05-1.json.gz\n",
      "Deleted 2015-01-05-1.json.gz\n",
      "Downloaded 2015-01-05-3.json.gz\n",
      "Deleted 2015-01-05-3.json.gz\n",
      "Downloaded 2015-01-05-2.json.gz\n",
      "Downloaded 2015-01-05-4.json.gz\n",
      "Deleted 2015-01-05-2.json.gz\n",
      "Deleted 2015-01-05-4.json.gz\n",
      "Downloaded 2015-01-05-5.json.gz\n",
      "Deleted 2015-01-05-5.json.gz\n",
      "Downloaded 2015-01-05-9.json.gz\n",
      "Deleted 2015-01-05-9.json.gz\n",
      "Downloaded 2015-01-05-10.json.gz\n",
      "Deleted 2015-01-05-10.json.gz\n",
      "Downloaded 2015-01-05-11.json.gz\n",
      "Deleted 2015-01-05-11.json.gz\n",
      "Downloaded 2015-01-05-12.json.gz\n",
      "Deleted 2015-01-05-12.json.gz\n",
      "Downloaded 2015-01-05-13.json.gz\n",
      "Deleted 2015-01-05-13.json.gz\n",
      "Downloaded 2015-01-05-14.json.gz\n",
      "Deleted 2015-01-05-14.json.gz\n",
      "Downloaded 2015-01-05-15.json.gz\n",
      "Deleted 2015-01-05-15.json.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to combine daily files into a monthly file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Example usage: Process data for January 2015\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mdownload_and_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2015\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mdownload_and_process_data\u001b[0;34m(year, month)\u001b[0m\n\u001b[1;32m     48\u001b[0m days_in_month \u001b[38;5;241m=\u001b[39m monthrange(year, month)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m tasks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 51\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mday\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays_in_month\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhour\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from calendar import monthrange\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_and_process_file(year, month, day, hour, download_dir, result_dir):\n",
    "    url = f'https://data.gharchive.org/{year}-{month:02d}-{day:02d}-{hour}.json.gz'\n",
    "    filename = url.split('/')[-1]\n",
    "    file_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "            print(f'Downloaded {filename}')\n",
    "\n",
    "            # Extract actor URL and write to daily file\n",
    "            with gzip.open(file_path, 'rt', encoding='utf-8') as f, \\\n",
    "                 open(os.path.join(result_dir, f'{year}-{month:02d}-{day:02d}_actor_urls.txt'), 'a') as daily_file:\n",
    "                for line in f:\n",
    "                    event_data = json.loads(line)\n",
    "                    if 'actor' in event_data and 'url' in event_data['actor']:\n",
    "                        daily_file.write(event_data['actor']['url'] + '\\n')\n",
    "\n",
    "            # Delete the downloaded file\n",
    "            os.remove(file_path)\n",
    "            print(f'Deleted {filename}')\n",
    "        else:\n",
    "            print(f'Failed to download {filename}, status code: {response.status_code}')\n",
    "    except requests.RequestException as e:\n",
    "        print(f'Network-related error occurred for {filename}: {e}')\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f'JSON decoding failed for {filename}: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'An unexpected error occurred for {filename}: {e}')\n",
    "\n",
    "def download_and_process_data(year, month):\n",
    "    download_dir = f'gharchive_{year}-{month:02d}'\n",
    "    result_dir = f'results_{year}-{month:02d}'\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    days_in_month = monthrange(year, month)[1]\n",
    "    tasks = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        for day in range(1, days_in_month + 1):\n",
    "            for hour in range(24):\n",
    "                task = executor.submit(download_and_process_file, year, month, day, hour, download_dir, result_dir)\n",
    "                tasks.append(task)\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            task.result()\n",
    "        except Exception as e:\n",
    "            print(f'Error occurred while processing a task: {e}')\n",
    "\n",
    "    # Combine daily files into a single monthly file\n",
    "    try:\n",
    "        with open(os.path.join(result_dir, f'{year}-{month:02d}_actor_urls.txt'), 'w') as monthly_file:\n",
    "            for day in range(1, days_in_month + 1):\n",
    "                daily_file_path = os.path.join(result_dir, f'{year}-{month:02d}-{day:02d}_actor_urls.txt')\n",
    "                with open(daily_file_path, 'r') as daily_file:\n",
    "                    monthly_file.writelines(daily_file.readlines())\n",
    "    except Exception as e:\n",
    "        print(f'Failed to combine daily files into a monthly file: {e}')\n",
    "\n",
    "# Example usage: Process data for January 2015\n",
    "download_and_process_data(2015, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lets start with 2020, 2021, 2022, 2024\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
